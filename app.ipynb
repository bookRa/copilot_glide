{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch as th\n",
    "import os\n",
    "\n",
    "from glide_text2im.download import load_checkpoint\n",
    "from glide_text2im.model_creation import (\n",
    "    create_model_and_diffusion,\n",
    "    model_and_diffusion_defaults,\n",
    "    model_and_diffusion_defaults_upsampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cuda_device_options():\n",
    "    has_cuda = th.cuda.is_available()\n",
    "    device = th.device('cuda' if has_cuda else 'cpu')\n",
    "    options = model_and_diffusion_defaults()\n",
    "    options['use_fp16'] = has_cuda\n",
    "    options['timestep_respacing'] = '100'\n",
    "    return has_cuda, device, options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "has_cuda = th.cuda.is_available()\n",
    "device = th.device('cuda' if has_cuda else 'cpu')\n",
    "options = model_and_diffusion_defaults()\n",
    "options['use_fp16'] = has_cuda\n",
    "options['timestep_respacing'] = '100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cuda, device, options = init_cuda_device_options()\n",
    "model, diffusion = create_model_and_diffusion(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total base parameters 385030726\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "if has_cuda:\n",
    "    model.convert_to_fp16()\n",
    "model.to(device)\n",
    "model.load_state_dict(load_checkpoint('base', device))\n",
    "print('total base parameters', sum(x.numel() for x in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |     806 MB |    2293 MB |    2293 MB |    1486 MB |\n",
      "|       from large pool |     765 MB |    2226 MB |    2226 MB |    1461 MB |\n",
      "|       from small pool |      40 MB |      66 MB |      66 MB |      25 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |     806 MB |    2293 MB |    2293 MB |    1486 MB |\n",
      "|       from large pool |     765 MB |    2226 MB |    2226 MB |    1461 MB |\n",
      "|       from small pool |      40 MB |      66 MB |      66 MB |      25 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    2334 MB |    2334 MB |    2334 MB |       0 B  |\n",
      "|       from large pool |    2266 MB |    2266 MB |    2266 MB |       0 B  |\n",
      "|       from small pool |      68 MB |      68 MB |      68 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    7851 KB |  533869 KB |    2218 MB |    2210 MB |\n",
      "|       from large pool |    6232 KB |  527222 KB |    2157 MB |    2151 MB |\n",
      "|       from small pool |    1619 KB |    8396 KB |      60 MB |      59 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     783    |    1566    |    1566    |     783    |\n",
      "|       from large pool |     192    |     422    |     422    |     230    |\n",
      "|       from small pool |     591    |    1144    |    1144    |     553    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     783    |    1566    |    1566    |     783    |\n",
      "|       from large pool |     192    |     422    |     422    |     230    |\n",
      "|       from small pool |     591    |    1144    |    1144    |     553    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     140    |     140    |     140    |       0    |\n",
      "|       from large pool |     106    |     106    |     106    |       0    |\n",
      "|       from small pool |      34    |      34    |      34    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       7    |      47    |     207    |     200    |\n",
      "|       from large pool |       2    |      35    |     156    |     154    |\n",
      "|       from small pool |       5    |      13    |      51    |      46    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(th.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.cuda.empty_cache()\n",
    "prompt = \"a bird on a rocket\"\n",
    "batch_size = 3\n",
    "guidance_scale = 3.0\n",
    "\n",
    "upsample_temp = 0.997\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text tokens to feed to the model.\n",
    "tokens = model.tokenizer.encode(prompt)\n",
    "tokens, mask = model.tokenizer.padded_tokens_and_mask(tokens, options['text_ctx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_batch_size = batch_size *2\n",
    "uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask([], options['text_ctx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    tokens = th.tensor([tokens] * batch_size + [uncond_tokens] * batch_size, device = device),\n",
    "    mask = th.tensor([mask] * batch_size + [uncond_mask] * batch_size, device=device, dtype=th.bool),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier-free guidance sampling function\n",
    "\n",
    "def model_fn(x_t, ts, **kwargs):\n",
    "    half = x_t[: len(x_t) //2]\n",
    "    combined = th.cat([half, half], dim = 0)\n",
    "    model_out = model(combined, ts, **kwargs)\n",
    "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "    cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim = 0)\n",
    "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
    "    eps = th.cat([half_eps, half_eps], dim = 0)\n",
    "    return th.cat([eps, rest], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a61b2da4b5460ca0a92163dc5c45f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.del_cache()\n",
    "samples = diffusion.p_sample_loop(\n",
    "    model_fn,\n",
    "    (full_batch_size, 3, options['image_size'], options['image_size']),\n",
    "    device = device,\n",
    "    clip_denoised=True,\n",
    "    progress=True,\n",
    "    model_kwargs=model_kwargs,\n",
    "    cond_fn=None\n",
    ")[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0369b954be93a71101cffefe9df9d757523104a20d6cb413ee43d4c726b785c9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('glide': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
